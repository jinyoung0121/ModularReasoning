dataset:                                            # Dataset configuration
    dataset_name: 'MyDataset'                       # Dataset name
    data_path: 'data'                               # Dataset path
    split: ''                                       # Dataset split. If '', it assumes there is only one split
    max_samples:                                    # Maximum number of samples to load
    start_sample: 0                                 # Start sample index. Only used if max_samples is not None

llama:
    model_path: ./pretrained_model/Meta-Llama-3.1-8B-Instruct
    max_batch_size: 4
    max_tokens: 2048
    do_sample: True
    temperature: 0.6
    top_p: 0.9

internlm:
    model_path: ./pretrained_model/internlm2_5-7b-chat
    max_batch_size: 3
    max_new_tokens: 2048
    do_sample: False
    temperature: 0.8
    top_p: 0.8

qwen:
    model_path_7b: ./pretrained_model/Qwen2.5-7B-Instruct
    model_path_14b: ./pretrained_model/Qwen2.5-14B-Instruct
    model_size: 7b
    max_batch_size: 3
    max_new_tokens: 512
    do_sample: False

blip:
    model_path: ./pretrained_model/blip2-flan-t5-xxl

internvl:
    model_path: ./pretrained_model/InternVL2-8B
    max_batch_size: 6

internlmxcomposer:
    model_path: ./pretrained_model/internlm-xcomposer2-vl-7b
    max_batch_size: 10

internvideo:
    model_path: ./pretrained_model/InternVideo2-Chat-8B

qd_detr:
    model_checkpoint_path: ./pretrained_model/QD_DETR/run_on_video/qd_detr_ckpt/model_best.ckpt
    clip_model: ViT-B/32

univtg:
    model_checkpoint_path: ./pretrained_model/UniVTG/results/omni/qvhl_pt/model_best.ckpt
    clip_model: ViT-B/32
    clip_vid_feat_path: video_clip_feats
    slowfast_vid_feat_path: video_slowfast_feats
    fps: 1

videollava:
    model_path: ./pretrained_model/Video-LLaVA-7B-hf
    max_batch_size: 3
    max_length: 512
    do_sample: False
    num_segments: 8
    num_return_sequences: 8
    top_k: 0
    top_p: 0.9
    temperature: 0.7

viclip:
    model_path: ./pretrained_model/ViCLIP-L-14-hf
    topk: -1

owlvit:
    model_path: ./pretrained_model/owlvit-base-patch32
    threshold: 0.1

clip:
    model_path: RN50
    threshold: 0.7

# Saving and loading parameters
save: True                                          # Save the results to a file
results_dir: ./results/                             # Directory to save the results
batch_size: 20                                      # Batch size : iter nums = len(data) // (world_size * batchs_size) 
log_freq: 10                                        # Log accuracy every n batches : 
exp_name: run                                       # experiment name
seed: 42                                            # set seed for reproducibility

mode: morevqa
llm_type: internlm                                  # which llm to use. internvl, qwen
image_vlm_type: internlmxcomposer                   # which vlm to use. internvl, internlmxcomposer, internvideo etc
video_vlm_type: videollava
retrieve_type: univtg                               # which retrieve model to use. univtg, qddetr, etc
question_type: mc                                   # either mc(multiple-choice) or oe(open-ended)
video_context: ./datas/NExT-QA/nextqa/internlmxcomposer2_brief_val.json
video_context_vid: ./datas/NExT-QA/nextqa/videollava_global_val.json
window_expand_ratio: 1.5                            # temporal window expand ratio
frame_id_selection_num: -1                          # number of selected frame id for imageQA in reasoning stage (only in ours:morevqa_understanding), -1 indicates deactivated